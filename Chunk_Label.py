import re
import json
import uuid
from pathlib import Path

CHUNK_SIZE = 900
CHUNK_OVERLAP = 0.15
INPUT_PATH = Path("data/cleaned_data/vector_data_haiku_processed_resume.jsonl")
OUTPUT_PATH = Path("data/cleaned_data/chunked_label.jsonl")

# âœ… ì¤‘ë³µëœ ì¢…ê²°ì–´ë¯¸ ìë™ ì •ì œ í•¨ìˆ˜
def clean_redundant_endings(text: str) -> str:
    """
    'ìˆìŠµë‹ˆë‹¤ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤í•©ë‹ˆë‹¤', 'ì„ í˜¸í•©ë‹ˆë‹¤í•©ë‹ˆë‹¤' ë“±
    ì¢…ê²° ì–´ë¯¸ ì¤‘ë³µ íŒ¨í„´ì„ ì œê±°í•˜ê³  ë¬¸ì¥ ë¶€ë“œëŸ½ê²Œ ì •ì œ.
    """
    text = re.sub(r'([ê°€-í£]+ìŠµë‹ˆë‹¤)\s*\1', r'\1', text)
    text = re.sub(r'([ê°€-í£]+ìŠµë‹ˆë‹¤)\s*ì…ë‹ˆë‹¤', r'\1', text)
    text = re.sub(r'([ê°€-í£]+í–ˆìŠµë‹ˆë‹¤)\s*ì…ë‹ˆë‹¤', r'\1', text)
    text = re.sub(r'([ê°€-í£]+í–ˆìŠµë‹ˆë‹¤)\s*\1', r'\1', text)
    text = re.sub(r'([ê°€-í£]+í–ˆë‹¤)\s*\1', r'\1', text)
    text = re.sub(r'([ê°€-í£]+ë‹¤)\s*\1', r'\1', text)

    # 2ï¸âƒ£ 'ë‹¤ë‹¤.' ê°™ì€ ì§§ì€ ì¤‘ë³µ ì œê±°
    text = re.sub(r'(ë‹¤)\1(\.|$)', r'\1\2', text)

    # 3ï¸âƒ£ ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def sentence_split(text: str):
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return [s.strip() for s in sentences if s.strip()]


def recursive_chunk(sentences, chunk_size=900, overlap=0.15):
    chunks = []
    current_chunk = []
    for sent in sentences:
        joined = " ".join(current_chunk + [sent])
        if len(joined) <= chunk_size:
            current_chunk.append(sent)
        else:
            chunks.append(" ".join(current_chunk))
            overlap_count = max(1, int(len(current_chunk) * overlap))
            current_chunk = current_chunk[-overlap_count:]
            current_chunk.append(sent)
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks


def chunk_and_label(record):
    # ì›ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°
    raw_text = str(record.get("answer_text", "")).strip()

    # âœ… ì¤‘ë³µëœ ì–´ë¯¸ ì •ì œ (1ì°¨)
    raw_text = clean_redundant_endings(raw_text)

    sentences = sentence_split(raw_text)
    chunks = recursive_chunk(sentences, CHUNK_SIZE, CHUNK_OVERLAP)

    for text in chunks:
        # âœ… ì²­í¬ë³„ë¡œë„ ì¤‘ë³µ ì–´ë¯¸ ì¬ì •ì œ (2ì°¨ ë³´ì •)
        cleaned_text = clean_redundant_endings(text)

        yield {
            "vector_uuid": str(uuid.uuid4()),
            "panel_uuid": record.get("panel_uuid"),
            "response_uuid": record.get("response_uuid"),
            "answer_text": cleaned_text,
            "embedding": None
        }


if __name__ == "__main__":
    print("ğŸ”¹ ì²­í‚¹ + ë¼ë²¨ë§ + ë¬¸ì¥ ì •ì œ ì‹œì‘")
    if not INPUT_PATH.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INPUT_PATH}")

    records = []
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # ë¹ˆ ì¤„ì€ ë¬´ì‹œ
                records.append(json.loads(line))

    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    total_chunks = 0
    with open(OUTPUT_PATH, "w", encoding="utf-8") as out_f:
        for record in records:
            for chunk in chunk_and_label(record):
                out_f.write(json.dumps(chunk, ensure_ascii=False) + "\n")
                total_chunks += 1

    print(f"âœ… ì´ {len(records)}ê°œ record ì²˜ë¦¬ ì™„ë£Œ")
    print(f"âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH.resolve()}")
